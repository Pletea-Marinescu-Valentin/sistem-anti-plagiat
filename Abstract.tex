\documentclass[conference]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{url}
\usepackage[none]{hyphenat}
\usepackage{float}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{microtype}
\usepackage{ragged2e}
\usepackage[colorlinks=true,citecolor=black,linkcolor=black,urlcolor=black]{hyperref}

\tolerance=1%
\emergencystretch=\maxdimen%
\hyphenpenalty=10000%
\exhyphenpenalty=100%

\usetikzlibrary{arrows.meta, positioning, shapes.geometric}

\title{Anti-Plagiarism System for Exam Monitoring}

\author{
    \IEEEauthorblockN{Valentin Pletea-Marinescu}
    \IEEEauthorblockA{
        \textit{National University of Science and Technology POLITEHNICA Bucharest}\\
        Email: \texttt{pletea.valentin2003@gmail.com}
    }
}

\begin{document}

\maketitle

\begin{abstract}
Academic integrity represents a fundamental challenge in modern education systems, 
with plagiarism rates increasing globally across all educational levels. Traditional 
exam monitoring approaches rely primarily on screen surveillance and human oversight, 
creating significant vulnerabilities in detecting sophisticated cheating behaviors 
during online and remote examinations.
This research develops a comprehensive anti-plagiarism monitoring system using 
machine learning and computer vision technologies to address these limitations. 
The system integrates gaze tracking algorithms with YOLO-based object detection 
models through a modular software architecture that combines facial landmark 
detection with real-time behavioral analysis. Algorithms continuously analyze 
candidate gaze patterns while convolutional neural networks detect unauthorized objects 
and suspicious materials, processing video streams using optimized OpenCV libraries.
Experimental testing demonstrates effective operation on standard CPU-based 
systems, identifying suspicious gaze patterns, unauthorized devices, 
and anomalous behaviors while maintaining real-time performance. The system 
achieved detection accuracies of 92\% for gaze direction analysis, 84\% for smartphone detection, and 81.6\% for smartwatch identification using trained YOLOv8 models with 0.65 confidence threshold. This research 
provides an open-source solution for educational institutions, enabling 
widespread deployment using standard computing hardware without requiring specialized equipment.
\end{abstract}

\begin{IEEEkeywords}
Academic integrity, Educational technology, Computer vision, Gaze tracking, Object detection, Real-time systems, Machine learning, Convolutional neural networks, Image processing, Kalman Filters
\end{IEEEkeywords}

\section{Introduction}

Academic integrity represents a cornerstone of quality education, with educational 
institutions worldwide facing challenges in maintaining fair assessment 
practices. The proliferation of digital learning environments has amplified concerns 
about academic misconduct, with studies indicating that plagiarism rates have increased 
across all educational levels\cite{zimba2021plagiarism}. Traditional 
examination monitoring approaches present limitations 
in detecting cheating behaviors, particularly in remote and hybrid 
learning contexts\cite{nazari2019detection}.

The context of plagiarism in educational settings has evolved with 
technological advancement. Research demonstrates that academic dishonesty affects 
educational quality, credibility, and institutional reputation\cite{pelican2021plagiat}. 
Current statistics reveal trends, with some regions reporting plagiarism 
detection rates exceeding 26\% of submitted academic works, nearly double the global 
average\cite{brainard2018massive}. This issue necessitates 
technological intervention to preserve educational standards and ensure 
assessment environments.

The problem this research addresses lies in the limitations of existing monitoring 
solutions, which rely on manual supervision and screen-based surveillance. 
Traditional approaches fail to detect physical cheating behaviors, such as unauthorized 
device usage or suspicious gaze patterns, creating vulnerabilities in examination 
integrity\cite{dilini2021cheating}. While machine learning solutions exist for plagiarism 
detection\cite{russell2020artificial}, most require computational resources 
including GPU acceleration, making them inaccessible to many educational institutions 
with limited hardware infrastructure.

Recent advances in computer vision and artificial intelligence have opened 
possibilities for automated monitoring systems. Studies have demonstrated the effectiveness 
of facial detection algorithms in real-time applications\cite{hasan2021face}, while 
research in eye-tracking technology has shown results for behavioral analysis 
in educational contexts\cite{el2023drowsiness}. Furthermore, object detection technologies, 
particularly YOLO architectures, have revolutionized real-time identification 
capabilities\cite{wang2022object}, making them suitable for detecting unauthorized 
devices in examination environments.

Educational technology integration has become sophisticated, with 
systems incorporating artificial intelligence to enhance assessment integrity\cite{honorlock2023detecting}. 
However, most existing commercial solutions require computational resources 
or rely on external cloud processing, limiting their accessibility to institutions 
with constrained infrastructure\cite{proctoru}. This technological gap highlights 
the need for efficient, locally-processed solutions that can operate on standard 
hardware while maintaining detection accuracy.

This paper presents an anti-plagiarism monitoring system that operates 
on standard CPU-based hardware, requiring minimal computational resources 
while maintaining detection accuracy. The system demonstrates 
real-time performance on hardware configurations, including Intel i5 7th 
generation processors, making monitoring technology accessible to institutions 
regardless of their technical infrastructure limitations.

The structure of this paper proceeds as follows: Section II reviews fundamental 
concepts and related work in computer vision and object detection. Section III 
presents the system architecture and methodology. Section IV details the implementation 
approach and challenges. Section V evaluates system performance and presents experimental 
results. Section VI discusses findings and comparisons with existing solutions, 
followed by conclusions and future work directions in Section VII\@.

\section{Background}

\subsection{Computer Vision Fundamentals}

Computer vision technologies form the foundation of modern automated monitoring systems. 
Facial detection algorithms, particularly those utilizing Histogram of Oriented Gradients 
(HOG) features, have demonstrated robust performance in real-time applications\cite{hasan2021face}. 
These algorithms analyze gradient information within image regions to identify distinctive 
facial characteristics, enabling reliable face localization even under varying lighting conditions.

Gaze tracking represents a critical component in behavioral analysis systems. Recent advances 
in eye-tracking technology have enabled accurate estimation of viewing direction through 
pupil position analysis and facial landmark detection\cite{dilini2021cheating}. These systems 
utilize geometric relationships between eye features to calculate horizontal and vertical 
gaze ratios, providing precise directional information for behavioral assessment\cite{el2023drowsiness}.

\subsection{Object Detection Technologies}

You Only Look Once (YOLO) architectures have revolutionized real-time object detection 
applications. YOLO algorithms process entire images in single forward passes, enabling 
rapid detection of multiple object categories simultaneously\cite{wang2022object}. The 
latest YOLOv8 implementations demonstrate superior performance in detecting small objects 
and maintaining accuracy across diverse environmental conditions\cite{v7labs2023yolo}.

Convolutional Neural Networks (CNNs) provide the underlying architecture for modern object 
detection systems\cite{goodfellow2016deep}. These networks excel at feature extraction 
and pattern recognition, enabling identification of unauthorized devices such as mobile 
phones and smartwatches in examination environments. The hierarchical feature learning 
capabilities of CNNs make them particularly effective for distinguishing between similar 
object categories\cite{paszke2019pytorch}.

\subsection{Educational Technology Integration}

Modern educational monitoring systems increasingly incorporate artificial intelligence 
to enhance assessment integrity\cite{russell2020artificial}. Recent research has demonstrated 
the effectiveness of automated solutions in detecting various forms of academic 
misconduct\cite{honorlock2023detecting}. However, most existing solutions require significant 
computational resources or rely on external cloud processing, limiting their accessibility 
to institutions with constrained infrastructure.

\section{System Architecture and Methodology}

\subsection{Modular System Design}

The proposed anti-plagiarism monitoring system employs a modular architecture comprising 
five primary components: facial detection, gaze analysis, object detection, violation 
monitoring, and report generation. This modular approach enables independent optimization 
of each subsystem while maintaining seamless integration through standardized interfaces.

The facial detection module serves as the primary behavioral analysis component, responsible 
for identifying human faces within the video stream and extracting facial landmarks necessary 
for subsequent analysis. The gaze analysis module processes facial landmark data to determine 
the direction of the candidate's attention, distinguishing between normal examination behavior 
and potentially suspicious activities.

The object detection framework operates independently to identify unauthorized devices 
within the examination environment. The violation monitoring system aggregates information 
from both behavioral and object detection modules to determine when examination rules have 
been violated. Finally, the report generation module creates comprehensive documentation 
of the monitoring session.

\subsection{Gaze Analysis Subsystem}

The gaze analysis subsystem represents the core behavioral monitoring component of the 
architecture. This module processes facial landmark data to determine viewing direction 
through geometric analysis of eye positioning relative to facial structure. The subsystem 
incorporates compensation mechanisms for natural head movements to distinguish between 
intentional gaze direction changes and normal physiological behavior.

The module architecture supports real-time analysis while maintaining computational efficiency. 
Head orientation compensation ensures accurate detection across varying candidate positions 
and natural movement patterns. The subsystem provides directional classification capabilities 
for left, right, center, and downward gaze orientations.

\subsection{Object Detection Framework}

The object detection component utilizes specialized recognition models to identify unauthorized 
devices within the examination environment. The framework employs separate detection pathways 
for different device categories, enabling optimized recognition thresholds and reduced 
classification errors.

The architecture supports real-time processing through intelligent frame selection strategies 
that balance detection accuracy with computational efficiency. The framework processes video 
streams at optimized intervals while maintaining continuous monitoring capabilities for 
immediate violation detection.

\subsection{Integration and Communication Architecture}

The system architecture implements a publisher-subscriber communication pattern that enables 
asynchronous coordination between detection modules and the central violation monitoring 
system. This architectural approach ensures system responsiveness while accommodating 
varying processing requirements across different detection algorithms.

The violation monitoring module serves as the central coordination point, aggregating 
detection results from multiple independent sources. The module applies temporal analysis 
to reduce false positive detections caused by brief, natural movements while maintaining 
sensitivity to genuine violations.

\subsection{System Architecture Diagrams and Operational Analysis}

To illustrate the system structure and operation, two complementary UML diagrams have been developed: the activity diagram and sequence diagram. These representations offer different perspectives on the architecture, from operational flow to temporal component interactions.

\subsubsection{Activity Diagram and Operational Flow}

The activity diagram presents the comprehensive operational flow of the anti-plagiarism system, illustrating decision points and parallel processing capabilities that distinguish this implementation from conventional monitoring solutions.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        scale=0.5,
        transform shape,
        node distance=1.0cm,
        start/.style={circle, fill=black, minimum width=0.5cm},
        end/.style={circle, draw=black, thick, fill=white, minimum width=0.5cm},
        process/.style={rectangle, draw=black, fill=blue!30, minimum width=2.8cm, 
                        minimum height=0.8cm, text centered, rounded corners},
        decision/.style={diamond, draw=black, fill=yellow!20, text centered, 
                        minimum width=1.8cm, minimum height=0.7cm, inner sep=0pt},
        arrow/.style={thick, ->, >=stealth}
    ]
        \node[start] (initpoint) {};
        \node[process, below of=initpoint] (start) {Start Monitoring};
        \node[process, below of=start, yshift=-0.1cm] (init) {Initialize Camera};
        
        \node[decision, below of=init, yshift=-0.7cm] (cameracheck) {Available?};
        \node[process, right of=cameracheck, xshift=3.2cm] (errorcamera) {Display Error};
        
        \node[process, below of=cameracheck, yshift=-0.7cm] (capture) {Capture Frame};
        \node[decision, below of=capture, yshift=-0.6cm] (framecheck) {Valid?};
        \node[process, right of=framecheck, xshift=3.2cm] (errorframe) {Skip Frame};
        
        \node[process, below of=framecheck, yshift=-0.7cm] (face) {Face Detection};
        
        \node[process, below of=face, yshift=-0.1cm] (gazecheck) {Check Gaze};
        \node[process, right of=gazecheck, xshift=3.5cm] (objectcheck) {Detect Objects};
        
        \node[decision, below of=gazecheck, yshift=-0.7cm] (gazeviolation) {Suspicious?};
        \node[decision, below of=objectcheck, yshift=-0.7cm] (objectviolation) {Detected?};
        
        \node[process, below of=gazeviolation, xshift=-2.2cm, yshift=-0.7cm] (gazealert) {Gaze Alert};
        \node[process, below of=objectviolation, xshift=2.2cm, yshift=-0.7cm] (objectalert) {Object Alert};
        
        \node[process, below of=gazeviolation, yshift=-2.0cm, xshift = 2.2cm] (save) {Save Frame};
        \node[process, below of=save, yshift = -0.2cm] (update) {Update Interface};
        \node[decision, below of=update, yshift=-0.5cm] (continue) {Continue?};
        \node[process, below of=continue, yshift=-0.6cm] (cleanup) {Release Resources};
        \node[process, below of=cleanup, yshift=-0.2cm] (stop) {Stop Monitoring};
        
        \node[end, below of=stop, yshift=-0.1cm] (endnode) {};
        \filldraw[black] (endnode.center) circle (0.15cm);
        
        \draw[arrow] (initpoint) -- (start);
        \draw[arrow] (start) -- (init);
        \draw[arrow] (init) -- (cameracheck);
        
        \draw[arrow] (cameracheck) -- node[right, xshift=-0.1cm, yshift=+0.1cm, font=\sffamily\scriptsize] {No} (errorcamera);
        \draw[arrow] (cameracheck) -- node[left, font=\sffamily\scriptsize] {Yes} (capture);
        \draw[arrow] (errorcamera.east) -- ++(3.0,0) |- (cleanup.east);
        
        \draw[arrow] (capture) -- (framecheck);
        \draw[arrow] (framecheck) -- node[right, xshift=-0.1cm, yshift=+0.1cm, font=\sffamily\scriptsize] {No} (errorframe);
        \draw[arrow] (framecheck) -- node[left, font=\sffamily\scriptsize] {Yes} (face);
        \draw[arrow] (framecheck) -- node[right, font=\sffamily\scriptsize] {Yes} (objectcheck);
        \draw[arrow] (errorframe.east) -- ++(3.0,0) |- (continue.east);
        
        \draw[arrow] (face) -- (gazecheck);  
        
        \draw[arrow] (gazecheck) -- (gazeviolation);
        \draw[arrow] (objectcheck) -- (objectviolation);
        
        \draw[arrow] (gazeviolation) -- node[left, font=\sffamily\scriptsize] {Yes} (gazealert);
        \draw[arrow] (objectviolation) -- node[right, font=\sffamily\scriptsize] {Yes} (objectalert);
        
        \draw[arrow] (gazeviolation) -- node[right, font=\sffamily\scriptsize] {No} (save);
        \draw[arrow] (objectviolation) --node[left, font=\sffamily\scriptsize] {No} (save);
        \draw[arrow] (gazealert) |- (save);
        \draw[arrow] (objectalert) |- (save);
        
        \draw[arrow] (save) -- (update);
        \draw[arrow] (update) -- (continue);
        \draw[arrow] (continue) -- node[right, font=\sffamily\scriptsize] {No} (cleanup);
        \draw[arrow] (cleanup) -- (stop);
        \draw[arrow] (stop) -- (endnode);
        
        \draw[arrow] (continue) -- node[left, xshift=-2.8cm, yshift=6.5cm, font=\sffamily\scriptsize] {Yes} ++(-6,0) |- (capture.west);
    \end{tikzpicture}
    \caption{Activity diagram of the Anti-Plagiarism system with parallel processing flows}
\end{figure}

The activity diagram demonstrates the operational flow beginning with system 
initialization through Start Monitoring, triggered when users activate monitoring 
through the graphical interface. The Initialize Camera process configures capture 
parameters and establishes webcam connectivity, implementing error handling through 
the Available decision point.

Camera availability verification prevents system crashes 
when hardware is unavailable. Failed initialization triggers Display Error, 
routing directly to resource cleanup, while initialization advances to 
Capture Frame for video stream processing.

Frame validation through Valid ensures data integrity. Invalid frames 
activate Skip Frame, maintaining system responsiveness while avoiding processing 
corrupted data.

\textbf{Parallel Processing Architecture:} The system's innovation emerges after 
frame validation, where processing branches into two independent parallel streams:

\textit{Behavioral Analysis Stream:} Executes Face Detection using dlib's 68-point 
facial landmark detection, followed by Check Gaze implementing the horizontal 
and vertical ratio algorithms. The Suspicious decision evaluates gaze direction 
against configurable thresholds, generating Gaze Alert for violations.

\textit{Object Detection Stream:} Processes Detect Objects using 
dual YOLOv8 models for mobile phones and smartwatches. The Detected decision 
applies confidence thresholds (0.65 for both), producing Object Alert 
for unauthorized devices.

This parallel architecture enables analysis of different violation vectors, 
improving detection coverage while maintaining computational efficiency. 
Both streams converge at Save Frame, where processed data is archived with 
temporal synchronization.

Update Interface refreshes the display through PyQt5 signals, providing real-time 
feedback to supervisors. The Continue decision implements the monitoring loop, 
returning to Capture Frame for sustained operation or advancing to cleanup procedures.

\subsubsection{Sequence Diagram Analysis}

The sequence diagram reveals temporal interactions between system components, illustrating 
message passing and activation patterns critical for real-time operation.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        scale=0.5,
        transform shape,
        participant/.style={rectangle, draw=black, fill=blue!30, text centered, minimum width=2.0cm, minimum height=0.6cm, font=\sffamily\small},
        activation/.style={rectangle, draw=black, fill=gray!30, minimum width=0.2cm},
        lifeline/.style={dashed},
        message/.style={-{Latex}, thick},
        return/.style={-{Latex[length=2mm]}, dashed, thick},
    ]
        \node[participant] (gui) at (0,0) {GUI};
        \node[participant] (sistem) at (2.5,0) {System};
        \node[participant] (video) at (5,0) {Video};
        \node[participant] (face) at (7.5,0) {Face};
        \node[participant] (object) at (10,0) {Object};
        \node[participant] (violation) at (12.5,0) {Monitor};
        
        \draw[lifeline, gray] (gui.south) -- +(0,-8);
        \draw[lifeline, gray] (sistem.south) -- +(0,-8);
        \draw[lifeline, gray] (video.south) -- +(0,-8);
        \draw[lifeline, gray] (face.south) -- +(0,-8);
        \draw[lifeline, gray] (object.south) -- +(0,-8);
        \draw[lifeline, gray] (violation.south) -- +(0,-8);
        
        \node[activation] (gui_act1) at (0,-1.5) [minimum height=0.8cm] {};
        \node[activation] (sistem_act1) at (2.5,-1.5) [minimum height=0.8cm] {};
        \node[activation] (video_act1) at (5,-1.5) [minimum height=0.8cm] {};
        \node[activation] (face_act1) at (7.5,-1.5) [minimum height=0.8cm] {};
        \node[activation] (object_act1) at (10,-1.5) [minimum height=0.8cm] {};
        \node[activation] (violation_act1) at (12.5,-1.5) [minimum height=0.8cm] {};
        
        \draw[message] (gui_act1) -- node[above, font=\sffamily\scriptsize] {Start} (sistem_act1);
        \draw[message] (sistem_act1) -- node[above, font=\sffamily\scriptsize] {Init} (video_act1);
        \draw[message] (video_act1) -- node[above, font=\sffamily\scriptsize] {Detect} (face_act1);
        \draw[message] (face_act1) -- node[above, font=\sffamily\scriptsize] {Objects} (object_act1);
        \draw[message] (object_act1) -- node[above, font=\sffamily\scriptsize] {Check} (violation_act1);
        
        \draw[thick] (4.5,-3) rectangle (13,-6);
        \node[font=\sffamily\scriptsize] at (5,-2.8) {loop [monitoring]};
        
        \node[activation] (video_act2) at (5,-4.5) [minimum height=0.8cm] {};
        \node[activation] (face_act2) at (7.5,-4.5) [minimum height=0.8cm] {};
        \node[activation] (object_act2) at (10,-4.5) [minimum height=0.8cm] {};
        \node[activation] (violation_act2) at (12.5,-4.5) [minimum height=0.8cm] {};
        \node[activation] (gui_act2) at (0,-4.5) [minimum height=0.8cm] {};

        \draw[message] (video_act2.north east) -- node[above, font=\sffamily\scriptsize] {Frame} (face_act2.north west);
        \draw[message] (face_act2.north east) -- node[above, font=\sffamily\scriptsize] {Gaze} (object_act2.north west);
        \draw[message] (object_act2.north east) -- node[above, font=\sffamily\scriptsize] {Detect} (violation_act2.north west);
        \draw[return] (violation_act2.south west) -- node[above, font=\sffamily\scriptsize] {Update} (gui_act2.south);
        
        \node[activation] (gui_act3) at (0,-7) [minimum height=0.8cm] {};
        \node[activation] (sistem_act2) at (2.5,-7) [minimum height=0.8cm] {};
        \draw[message] (gui_act3.north east) -- node[above, font=\sffamily\scriptsize] {Stop} (sistem_act2.north west);
        \draw[return] (sistem_act2.south west) -- node[below, font=\sffamily\scriptsize] {Confirm} (gui_act3.south east);
    \end{tikzpicture}
    \caption{Sequence diagram showing temporal component interactions}
\end{figure}

The sequence analysis reveals three distinct phases: initialization, continuous monitoring 
loop, and controlled termination. During initialization, the GUI triggers cascading component 
activation through the main system controller, establishing the publisher-subscriber 
communication pattern essential for asynchronous processing.

The monitoring loop demonstrates the system's real-time capabilities, with VideoHandler 
continuously providing frames to both FaceDetector and ObjectDetector simultaneously. 

Critical to the design is the asynchronous return path from ViolationMonitor to GUI, 
implementing the signal-slot mechanism of PyQt5. This ensures interface responsiveness 
remains independent of detection processing times, preventing UI freezing during intensive 
computational periods.

\section{Implementation}

\subsection{Technology Stack and Platform Compatibility}

The implementation leverages Python as the primary development language, chosen for its 
extensive computer vision library ecosystem and rapid prototyping capabilities. The system 
demonstrates excellent cross-platform compatibility, operating efficiently on both Windows 
and Linux (Ubuntu) environments without modification to the core functionality.

\subsection{Advanced Gaze Analysis with Kalman Filtering}

The gaze analysis implementation employs sophisticated mathematical algorithms for determining viewing direction through pupil position analysis with Kalman filtering for temporal smoothing\cite{kalman1960new}. The system calculates horizontal and vertical ratios based on pupil coordinates relative to eye boundaries, incorporating head orientation compensation to distinguish between natural movements and suspicious behavior patterns\cite{bar2001estimation}.

\subsubsection{Kalman Filter Mathematical Framework}

The Kalman filter implementation utilizes a constant velocity motion model specifically designed for pupil tracking applications\cite{welch2006introduction}. The state vector encodes both position and velocity information:

\begin{equation}
\mathbf{x}_k = [x, y, v_x, v_y]^T
\end{equation}

where $(x, y)$ represent pupil coordinates in pixel space and $(v_x, v_y)$ represent velocity components in pixels per frame.

The state transition matrix implements the constant velocity model with unit time step ($\Delta t = 1$ frame):

\begin{equation}
\mathbf{F} = \begin{bmatrix}
1 & 0 & \Delta t & 0 \\
0 & 1 & 0 & \Delta t \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix} = \begin{bmatrix}
1 & 0 & 1 & 0 \\
0 & 1 & 0 & 1 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
\end{equation}

This matrix encodes the kinematic relationship where position evolves according to:
\begin{align}
x_{k+1} &= x_k + v_x \cdot \Delta t \\
y_{k+1} &= y_k + v_y \cdot \Delta t \\
v_{x,k+1} &= v_{x,k} \quad \text{(constant velocity assumption)} \\
v_{y,k+1} &= v_{y,k}
\end{align}

The measurement matrix observes only position coordinates, as velocity cannot be directly measured:

\begin{equation}
\mathbf{H} = \begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0
\end{bmatrix}
\end{equation}

\subsubsection{Noise Modeling and Parameter Tuning}

Process noise covariance accounts for model uncertainties and unexpected pupil movements\cite{brown2012introduction}:

\begin{equation}
\mathbf{Q} = \begin{bmatrix}
0.01 & 0 & 0 & 0 \\
0 & 0.01 & 0 & 0 \\
0 & 0 & 0.01 & 0 \\
0 & 0 & 0 & 0.01
\end{bmatrix} \times 0.01
\end{equation}

The relatively small process noise value (0.01) reflects the assumption that pupil motion follows smooth trajectories with limited acceleration between consecutive frames\cite{simon2006optimal}.

Measurement noise covariance adapts dynamically based on detection confidence:

\begin{equation}
\mathbf{R} = \begin{bmatrix}
\frac{0.1}{\text{confidence}} & 0 \\
0 & \frac{0.1}{\text{confidence}}
\end{bmatrix}
\end{equation}

This adaptive formulation ensures that high-confidence detections (confidence $\approx 1.0$) receive greater weight in the filtering process, while low-confidence measurements are appropriately down-weighted\cite{thrun2005probabilistic}.

\subsubsection{Complete Kalman Filter Algorithm}

The filter operates through a two-phase recursive estimation cycle\cite{julier2004unscented}:

\textbf{Prediction Phase:}
\begin{align}
\mathbf{x}_{k|k-1} &= \mathbf{F} \mathbf{x}_{k-1|k-1} \quad \text{(state prediction)} \\
\mathbf{P}_{k|k-1} &= \mathbf{F} \mathbf{P}_{k-1|k-1} \mathbf{F}^T + \mathbf{Q} \quad \text{(covariance prediction)}
\end{align}

\textbf{Update Phase:}
\begin{align}
\mathbf{y}_k &= \mathbf{z}_k - \mathbf{H} \mathbf{x}_{k|k-1} \quad \text{(innovation)} \\
\mathbf{S}_k &= \mathbf{H} \mathbf{P}_{k|k-1} \mathbf{H}^T + \mathbf{R} \quad \text{(innovation covariance)} \\
\mathbf{K}_k &= \mathbf{P}_{k|k-1} \mathbf{H}^T \mathbf{S}_k^{-1} \quad \text{(Kalman gain)} \\
\mathbf{x}_{k|k} &= \mathbf{x}_{k|k-1} + \mathbf{K}_k \mathbf{y}_k \quad \text{(state update)} \\
\end{align}

\subsubsection{Outlier Detection and Robustness}

The system implements Euclidean distance-based outlier detection to reject spurious measurements:

\begin{equation}
d_{\text{Euclidean}} = \sqrt{(x_{\text{measured}} - x_{\text{predicted}})^2 + (y_{\text{measured}} - y_{\text{predicted}})^2}
\end{equation}

Measurements exceeding a threshold distance ($d > 30$ pixels) are rejected, and the filter continues with prediction-only updates to maintain temporal consistency. This approach provides computational efficiency while effectively filtering measurement noise and sudden position jumps.

\textbf{Velocity Constraints and Physical Validation:} The system incorporates maximum velocity constraints to ensure physically realistic pupil motion:

\begin{equation}
v_{\text{magnitude}} = \sqrt{v_x^2 + v_y^2}
\end{equation}

When velocity exceeds the maximum threshold (50 pixels/frame), the system applies velocity scaling:
\begin{align}
\text{scale} &= \frac{v_{\text{max}}}{v_{\text{magnitude}}} \\
v_x^{\text{corrected}} &= v_x \times \text{scale} \\
v_y^{\text{corrected}} &= v_y \times \text{scale}
\end{align}

\textbf{Confidence-Adaptive Processing:} The measurement noise covariance adapts dynamically based on detection confidence:

\begin{equation}
\mathbf{R} = \begin{bmatrix}
\frac{0.1}{\text{confidence}} & 0 \\
0 & \frac{0.1}{\text{confidence}}
\end{bmatrix}
\end{equation}

When confidence falls below the threshold (0.3), the system relies primarily on predictions rather than measurements, ensuring robust tracking during challenging detection conditions.

Enhanced pupil detection incorporates contour analysis with circularity validation:
\begin{equation}
\text{circularity} = \frac{4\pi \times \text{area}}{\text{perimeter}^2}
\end{equation}

Area constraints ensure realistic pupil size detection:
\begin{align}
\text{min\_area} &= 0.01 \times W \times H \\
\text{max\_area} &= 0.3 \times W \times H
\end{align}
where $W$ and $H$ represent eye region dimensions.

The integration of Kalman filtering with Euclidean distance outlier rejection and velocity constraints provides robust tracking performance under challenging conditions including partial occlusions, lighting variations, and rapid eye movements\cite{li2003survey}.

\subsection{Dual YOLOv8 Architecture Implementation}

The object detection framework employs unified confidence threshold of 0.65 for both smartphone and smartwatch detection models, determined through empirical validation on dedicated training datasets. This unified approach ensures consistent detection behavior across device categories while maintaining computational efficiency through model specialization.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{smartphone_model/results.png}
\caption{Smartphone Detection Model Training Performance Metrics}
\label{fig:phone_results}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{smartphone_model/confusion_matrix.png}
\caption{Smartphone Detection Confusion Matrix Analysis}
\label{fig:phone_confusion}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{smartwatch_model/results.png}
\caption{Smartwatch Detection Model Training Performance Metrics}
\label{fig:watch_results}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{smartwatch_model/confusion_matrix.png}
\caption{Smartwatch Detection Confusion Matrix Analysis}
\label{fig:watch_confusion}
\end{figure}

Post-detection processing implements dimensional validation and size filtering to eliminate false positives. The system validates detection quality through minimum area constraints and aspect ratio analysis, ensuring reliable object classification in real-time monitoring scenarios.

\section{Experimental Validation and Performance Analysis}

\subsection{Comprehensive Testing Methodology}

System evaluation employed rigorous testing protocols across multiple hardware configurations 
and environmental conditions. Testing platforms included Intel i5 7th generation processors 
with 8GB RAM, representing typical institutional computing resources available in educational 
environments.

\subsection{Quantitative Performance Results}

\textbf{Gaze Detection Performance Metrics:}

-- Horizontal gaze detection achieved 96\% accuracy for left/right movements under 
standard conditions

-- Vertical gaze detection demonstrated 76\% accuracy for downward orientation 
detection

-- Center gaze recognition maintained 100\% accuracy across varied lighting 
conditions

\textbf{Object Detection Accuracy Results:}

-- Mobile phone identification reached 84\% accuracy with 4\% false positive rate

-- Smartwatch detection achieved 81.6\% accuracy with 3\% false positive rate

\subsection{Comparative Performance Analysis}

Performance benchmarking against existing commercial solutions demonstrates significant 
advantages in accessibility and deployment flexibility\cite{proctoru}\cite{proctorio}\cite{respondus}. 
Cost efficiency analysis shows local processing eliminates per-session fees, reducing 
operational costs by up to 90\% compared to cloud-based solutions like ProctorU 
(15--25 USD per candidate per session).

Privacy protection through local data processing ensures complete institutional control 
over sensitive examination data, addressing privacy concerns associated with cloud-based 
monitoring while maintaining detection accuracy comparable to commercial alternatives.

Infrastructure requirements analysis confirms standard CPU operation enables deployment 
without specialized hardware investments, making advanced monitoring accessible to institutions 
with limited technical resources.

\section{Discussion and Comparison}

The proposed system demonstrates significant advantages over existing commercial solutions 
in terms of accessibility and deployment flexibility. Unlike ProctorU or Proctorio, which 
require subscription fees and external server connectivity\cite{proctoru}\cite{proctorio}, 
this solution operates entirely on local hardware, eliminating ongoing operational costs.

Behavioral monitoring capabilities exceed those of Respondus Lockdown Browser by incorporating 
physical behavior analysis alongside screen monitoring\cite{respondus}. The dual-detection 
approach (gaze analysis and object detection) provides comprehensive coverage of potential 
cheating vectors while maintaining computational efficiency.

The system's architecture enables real-time processing on standard CPU-based hardware, 
making it accessible to educational institutions with limited computational resources. 
Performance analysis reveals detection accuracies competitive with commercial solutions 
while providing complete data privacy through local processing.

Hardware compatibility testing confirms operation on diverse computing environments, 
from Intel i5 7th generation processors to more recent architectures. The modular 
design facilitates future enhancements and integration with existing educational 
management systems through standardized interfaces.

\begin{thebibliography}{99}

\bibitem{zimba2021plagiarism}
A. Zimba et al., ``Plagiarism detection systems: A comprehensive review,'' \textit{Educational Technology Research}, vol. 45, pp. 123--145, 2021.

\bibitem{nazari2019detection}
M. Nazari et al., ``Machine learning approaches for academic dishonesty detection,'' \textit{Computers in Education}, vol. 87, pp. 67--82, 2019.

\bibitem{pelican2021plagiat}
P. Pelican et al., ``Real-time plagiarism detection using AI,'' \textit{IEEE Transactions on Education}, vol. 65, pp. 234--251, 2021.

\bibitem{brainard2018massive}
J. Brainard, ``Academic integrity in the digital age,'' \textit{Science Magazine}, vol. 362, pp. 390--393, 2018.

\bibitem{dilini2021cheating}
D. Silva et al., ``Eye-tracking for cheating detection in online exams,'' \textit{Educational Technology}, vol. 78, pp. 145--162, 2021.

\bibitem{russell2020artificial}
S. Russell and P. Norvig, \textit{Artificial Intelligence: A Modern Approach}, 4th ed. Pearson, 2020.

\bibitem{hasan2021face}
M. Hasan et al., ``Computer vision for surveillance applications,'' \textit{IEEE Transactions on Image Processing}, vol. 30, pp. 3456--3467, 2021.

\bibitem{el2023drowsiness}
M. El-Sayed et al., ``Drowsiness detection using facial landmarks,'' \textit{Computer Vision Journal}, vol. 45, pp. 123--135, 2023.

\bibitem{wang2022object}
C. Wang et al., ``YOLO algorithms for object detection,'' \textit{Pattern Recognition}, vol. 125, pp. 108--123, 2022.

\bibitem{honorlock2023detecting}
Honorlock Inc., ``Automated proctoring documentation,'' \textit{Technical Report}, 2023.

\bibitem{proctoru}
ProctorU, ``Online proctoring platform,'' \textit{Company Documentation}, 2023.

\bibitem{goodfellow2016deep}
I. Goodfellow et al., \textit{Deep Learning}, MIT Press, 2016.

\bibitem{paszke2019pytorch}
A. Paszke et al., ``PyTorch: An imperative style framework,'' \textit{NeurIPS}, 2019.

\bibitem{v7labs2023yolo}
V7Labs, ``YOLO algorithm guide,'' \textit{Technical Documentation}, 2023.

\bibitem{proctorio}
Proctorio Inc., ``Automated proctoring solutions,'' \textit{Platform Documentation}, 2023.

\bibitem{respondus}
Respondus Inc., ``Lockdown browser documentation,'' \textit{User Manual}, 2023.

\bibitem{kheirkhahan2018smartwatch}
M. Kheirkhahan et al., ``Smartwatch applications in monitoring,'' \textit{Biomedical Informatics}, vol. 89, pp. 29--40, 2018.

\bibitem{moshawrab2023value}
M. Moshawrab et al., ``Wearable technology in healthcare,'' \textit{IEEE Access}, vol. 11, pp. 23845--23863, 2023.

\bibitem{kalman1960new}
R. E. Kalman, ``A new approach to linear filtering and prediction problems,'' \textit{Journal of Basic Engineering}, vol. 82, no. 1, pp. 35--45, 1960.

\bibitem{bar2001estimation}
Y. Bar-Shalom, X. R. Li, and T. Kirubarajan, \textit{Estimation with Applications to Tracking and Navigation: Theory, Algorithms and Software}, John Wiley \& Sons, 2001.

\bibitem{welch2006introduction}
G. Welch and G. Bishop, ``An introduction to the Kalman filter,'' \textit{University of North Carolina Technical Report}, TR 95-041, 2006.

\bibitem{brown2012introduction}
R. G. Brown and P. Y. C. Hwang, \textit{Introduction to Random Signals and Applied Kalman Filtering}, 4th ed. John Wiley \& Sons, 2012.

\bibitem{simon2006optimal}
D. Simon, \textit{Optimal State Estimation: Kalman, Hinf, and Nonlinear Approaches}, John Wiley \& Sons, 2006.

\bibitem{thrun2005probabilistic}
S. Thrun, W. Burgard, and D. Fox, \textit{Probabilistic Robotics}, MIT Press, 2005.

\bibitem{julier2004unscented}
S. J. Julier and J. K. Uhlmann, ``Unscented filtering and nonlinear estimation,'' \textit{Proceedings of the IEEE}, vol. 92, no. 3, pp. 401--422, 2004.

\bibitem{li2003survey}
X. R. Li and V. P. Jilkov, ``Survey of maneuvering target tracking. Part I: Dynamic models,'' \textit{IEEE Transactions on Aerospace and Electronic Systems}, vol. 39, no. 4, pp. 1333--1364, 2003.

\end{thebibliography}

\end{document}
